{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTANT CODE SNIPPETS\n",
    "One stop destination for all code snippets,\n",
    "which are useful for processes in ML.\n",
    "\n",
    "The code gets updated from time to time as\n",
    "I learn more and more techniques and \n",
    "document them.\n",
    "\n",
    "Happy Machine Learning.\n",
    "We are the future.\n",
    "\n",
    "@author: Bikram Dutta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Frequecy in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['adrian', 'baby', 'rambo', 'rocky']\n[[0 1 0 2]\n [0 0 2 2]\n [2 0 1 1]]\n{'adrian': 2, 'baby': 1, 'rambo': 3, 'rocky': 5}\n{'baby': 1, 'rocky': 2}\n{'rambo': 2, 'rocky': 2}\n{'adrian': 2, 'rambo': 1, 'rocky': 1}\n"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "sampleTextList = [['rocky','baby','rocky'], ['rocky','rocky', 'rambo', 'rambo'], ['adrian', 'rocky', 'adrian', 'rambo']]\n",
    "\n",
    "\n",
    "cv = CountVectorizer()\n",
    "newList = []\n",
    "for sampleList in sampleTextList:\n",
    "    newList.append(' '.join(sampleList))\n",
    "    \n",
    "sampleTextList = newList\n",
    "\n",
    "# for just count matrix\n",
    "cv_fit = cv.fit_transform(sampleTextList)\n",
    "\n",
    "print(cv.get_feature_names())\n",
    "print(cv_fit.toarray())\n",
    "\n",
    "wordList = cv.get_feature_names()\n",
    "countList = cv_fit.toarray().sum(axis=0)\n",
    "wordCountMap = dict(zip(wordList,countList))\n",
    "\n",
    "print(wordCountMap)\n",
    "\n",
    "# for every text in the list\n",
    "\n",
    "for text in sampleTextList:\n",
    "    cv = CountVectorizer()\n",
    "    cv_fit = cv.fit_transform(text.split())\n",
    "    wordList = cv.get_feature_names()\n",
    "    countList = cv_fit.toarray().sum(axis=0)\n",
    "    wordCountMap = dict(zip(wordList,countList))\n",
    "    print(wordCountMap)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Original Numpy Array :  ['rocky baby rocky' 'rocky rocky rambo rambo' 'adrian rocky adrian rambo']\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'adrian rocky adrian rambo': 1,\n 'rocky baby rocky': 1,\n 'rocky rocky rambo rambo': 1}"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "import numpy\n",
    "arr = numpy.array(newList)\n",
    "print('Original Numpy Array : ' , arr)\n",
    " \n",
    "# Get a tuple of unique values & their frequency in numpy array\n",
    "uniqueValues, occurCount = numpy.unique(arr, return_counts=True)\n",
    "\n",
    "valueDict = dict(zip(uniqueValues, occurCount))\n",
    " \n",
    "# print(\"Unique Values : \" , uniqueValues)\n",
    "# print(\"Occurrence Count : \", occurCount)\n",
    "valueDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['rocky',\n 'baby',\n 'rocky',\n 'rocky',\n 'rocky',\n 'rambo',\n 'rambo',\n 'adrian',\n 'rocky',\n 'adrian',\n 'rambo']"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "sampleList = [['rocky','baby','rocky'], ['rocky','rocky', 'rambo', 'rambo'], ['adrian', 'rocky', 'adrian', 'rambo']]\n",
    "newList = []\n",
    "for sample in sampleList:\n",
    "    newList.extend(sample)\n",
    "    \n",
    "newList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "     Word  Count\n0  adrian      2\n1   rambo      1\n2   rocky      1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Word</th>\n      <th>Count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>adrian</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>rambo</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>rocky</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "wordCountDf = pd.DataFrame(list(zip(wordList,countList)),columns =['Word', 'Count']) \n",
    "wordCountDf.sort_values(by=['Count'], inplace=True, ascending = False)\n",
    "wordCountDf.reset_index(inplace=True, drop=True)\n",
    "wordCountDf\n",
    "# list(wordCountDf['Word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['rocky baby rocky', 'rocky rocky rambo rambo', 'adrian rocky adrian rambo']\n"
    }
   ],
   "source": [
    "sampleTextList = [['rocky baby rocky', 'rocky rocky rambo rambo'], ['adrian rocky adrian rambo']]\n",
    "wordList = []\n",
    "list(map(wordList.extend, sampleTextList))\n",
    "print(wordList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[1.         0.63245553 0.36514837]\n [0.63245553 1.         0.57735027]\n [0.36514837 0.57735027 1.        ]]\n"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "sampleTextList = ['rocky baby rocky', 'rocky rocky rambo rambo', 'adrian rocky adrian rambo']\n",
    "cv = CountVectorizer()\n",
    "\n",
    "# for just count matrix\n",
    "cv_fit = cv.fit_transform(sampleTextList)\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarity_scores = cosine_similarity(cv_fit)\n",
    "\n",
    "print(similarity_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n-gram Containment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 5, 'is': 2, 'an': 0, 'answer': 1, 'text': 4, 'source': 3}\n",
      "{'this is': 5, 'is an': 2, 'an answer': 0, 'answer text': 1, 'is source': 3, 'source text': 4}\n",
      "  (0, 5)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 0)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 4)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 3)\t1\n",
      "[[1 1 1 0 1 1]\n",
      " [0 0 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "a_text = \"This is an answer text\"\n",
    "s_text = \"This is a source text\"\n",
    "\n",
    "# set n\n",
    "n = 1\n",
    "\n",
    "# instantiate an ngram counter\n",
    "    counts = CountVectorizer(analyzer='word', ngram_range=(n,n))\n",
    "\n",
    "# create a dictionary of n-grams by calling `.fit`\n",
    "vocab2int = counts.fit([a_text, s_text]).vocabulary_\n",
    "\n",
    "# print dictionary of words:index\n",
    "print(vocab2int)\n",
    "\n",
    "# create a vocabulary for 2-grams\n",
    "# counts_2grams = None\n",
    "    counts_2grams = CountVectorizer(analyzer='word', ngram_range=(n+1,n+1))\n",
    "# create a dictionary of 2-grams by calling `.fit`\n",
    "vocabFor2grams = counts_2grams.fit([a_text, s_text]).vocabulary_\n",
    "print(vocabFor2grams)\n",
    "\n",
    "\n",
    "# create array of n-gram counts for the answer and source text\n",
    "ngrams = counts.fit_transform([a_text, s_text])\n",
    "print(ngrams)\n",
    "# row = the 2 texts and column = indexed vocab terms (as mapped above)\n",
    "# ex. column 0 = 'an', col 1 = 'answer'.. col 4 = 'text'\n",
    "ngram_array = ngrams.toarray()\n",
    "print(ngram_array)\n",
    "\n",
    "def containment(ngram_array):\n",
    "    ''' Containment is a measure of text similarity. It is the normalized, \n",
    "       intersection of ngram word counts in two texts.\n",
    "       :param ngram_array: an array of ngram counts for an answer and source text.\n",
    "       :return: a normalized containment value.'''\n",
    "    temp = list(ngram_array)\n",
    "    count = len(temp[0])\n",
    "    commonTerms = 0\n",
    "    commonIndexes = []\n",
    "    while count>0:\n",
    "        index = len(temp[0]) - count\n",
    "        if temp[0][index] == temp[1][index]:\n",
    "            commonTerms +=1\n",
    "            commonIndexes.append(index)\n",
    "        count = count -1 \n",
    "    \n",
    "    containment = commonTerms / np.count_nonzero(temp[0] > 0)\n",
    "    \n",
    "    # your code here\n",
    "    \n",
    "    return containment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'containment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-78c5079c4852>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# test out your code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcontainment_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontainment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mngrams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Containment: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontainment_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'containment' is not defined"
     ]
    }
   ],
   "source": [
    "# test out your code\n",
    "containment_val = containment(ngrams.toarray())\n",
    "\n",
    "print('Containment: ', containment_val)\n",
    "\n",
    "# note that for the given texts, and n = 1\n",
    "# the containment value should be 3/5 or 0.6\n",
    "assert containment_val==0.6, 'Unexpected containment value for n=1.'\n",
    "print('Test passed!')\n",
    "\n",
    "\n",
    "# test for n = 2\n",
    "counts_2grams = CountVectorizer(analyzer='word', ngram_range=(2,2))\n",
    "bigram_counts = counts_2grams.fit_transform([a_text, s_text])\n",
    "\n",
    "# calculate containment\n",
    "containment_val = containment(bigram_counts.toarray())\n",
    "\n",
    "print('Containment for n=2 : ', containment_val)\n",
    "\n",
    "# the containment value should be 1/4 or 0.25\n",
    "assert containment_val==0.25, 'Unexpected containment value for n=2.'\n",
    "print('Test passed!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1\n",
    "answer_filename = 'g0pA_taska.txt'\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "counts = CountVectorizer(analyzer='word', ngram_range=(n,n))\n",
    "vocab = counts.fit(list(complete_df.Text)).vocabulary_\n",
    "# print(vocab)\n",
    "ngrams = counts.fit_transform(list(complete_df.Text))\n",
    "# print(ngrams)\n",
    "ngram_array = ngrams.toarray()\n",
    "\n",
    "\n",
    "complete_df['ngrams'] = list(ngram_array)\n",
    "\n",
    "def getSourceNgram(complete_df,answer_filename):\n",
    "    task = list(complete_df[complete_df.File == answer_filename]['Task'])[0]\n",
    "    print(task)\n",
    "    source_ngram = list(complete_df[(complete_df['Task'] == task) & (complete_df.Class == -1)].ngrams)[0].tolist()\n",
    "#     print('Source ngram: ', source_ngram)\n",
    "    return source_ngram\n",
    "\n",
    "def getNgramsForText(complete_df,answer_filename):\n",
    "    return list(complete_df[complete_df.File == answer_filename].ngrams)[0].tolist()\n",
    "\n",
    "resultant = []\n",
    "for element in ngram_array:\n",
    "    if resultant == []:\n",
    "        resultant = element\n",
    "    \n",
    "    else:\n",
    "        resultant = np.bitwise_and(resultant,element)\n",
    "\n",
    "globalCommonTerms = np.count_nonzero(resultant>0)\n",
    "\n",
    "ngramsForAnswer = getNgramsForText(complete_df,answer_filename)\n",
    "ngramsForSource = getSourceNgram(complete_df,answer_filename)\n",
    "commonbetweenSrcAndAns = np.bitwise_and(np.array(ngramsForAnswer),np.array(ngramsForSource))\n",
    "\n",
    "containment = np.count_nonzero(commonbetweenSrcAndAns > 0) / np.count_nonzero(np.array(ngramsForAnswer) > 0)\n",
    "\n",
    "print(len(commonbetweenSrcAndAns),len(ngramsForSource), len(resultant))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_array = [[1,1,1,0,1],\n",
    "              [1,0,1,1,1],\n",
    "              [1,1,0,0,1],\n",
    "              [1,1,1,1,1]]\n",
    "\n",
    "temp = list(ngram_array)\n",
    "count = len(temp[0])\n",
    "commonTerms = 0\n",
    "commonIndexes = []\n",
    "while count>0:\n",
    "    index = len(temp[0]) - count\n",
    "    if temp[0][index] == temp[1][index]:\n",
    "        commonTerms +=1\n",
    "        commonIndexes.append(index)\n",
    "    count = count -1 \n",
    "\n",
    "containment = commonTerms / np.count_nonzero(temp[0] > 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\InstalledPrograms\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "ngram_array = [[1,1,1,0,1],\n",
    "              [1,0,1,1,1],\n",
    "              [1,1,0,0,1],\n",
    "              [1,1,1,1,1]]\n",
    "\n",
    "resultant = []\n",
    "for element in ngram_array:\n",
    "    if resultant == []:\n",
    "        resultant = element\n",
    "    \n",
    "    else:\n",
    "        resultant = np.bitwise_and(resultant,element)\n",
    "        \n",
    "np.count_nonzero(resultant>0)\n",
    "\n",
    "# np.bitwise_and(ngram_array[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a list of categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of categorical variables\n",
    "s = (X_train.dtypes == 'object')\n",
    "object_cols = list(s[s].index)\n",
    "\n",
    "print(\"Categorical variables:\")\n",
    "print(object_cols)\n",
    "\n",
    "# Get number of unique entries in each column with categorical data\n",
    "object_nunique = list(map(lambda col: X_train[col].nunique(), object_cols))\n",
    "d = dict(zip(object_cols, object_nunique))\n",
    "\n",
    "# Print number of unique entries by column, in ascending order\n",
    "sorted(d.items(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply one-hot encoder to each column with categorical data\n",
    "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[low_cardinality_cols]))\n",
    "OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[low_cardinality_cols]))\n",
    "\n",
    "# One-hot encoding removed index; put it back\n",
    "OH_cols_train.index = X_train.index\n",
    "OH_cols_valid.index = X_valid.index\n",
    "\n",
    "# Remove categorical columns (will replace with one-hot encoding)\n",
    "num_X_train = X_train.drop(object_cols, axis=1)\n",
    "num_X_valid = X_valid.drop(object_cols, axis=1)\n",
    "\n",
    "# Add one-hot encoded columns to numerical features\n",
    "OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n",
    "OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}